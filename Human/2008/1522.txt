Broadcasting with side information

A sender holds a word x consisting of n blocks x_i, each of t bits, and wishes to broadcast a codeword to m receivers, R_1,...,R_m. Each receiver R_i is interested in one block, and has prior side information consisting of some subset of the other blocks. Let β_t be the minimum number of bits that has to be transmitted when each block is of length t, and let βbe the limit β= \lim_{t \to \infty} β_t/t. In words, βis the average communication cost per bit in each block (for long blocks). Finding the coding rate β, for such an informed broadcast setting, generalizes several coding theoretic parameters related to Informed Source Coding on Demand, Index Coding and Network Coding.
  In this work we show that usage of large data blocks may strictly improve upon the trivial encoding which treats each bit in the block independently. To this end, we provide general bounds on β_t, and prove that for any constant C there is an explicit broadcast setting in which β= 2 but β_1 > C. One of these examples answers a question of Lubetzky and Stav.
  In addition, we provide examples with the following counterintuitive direct-sum phenomena. Consider a union of several mutually independent broadcast settings. The optimal code for the combined setting may yield a significant saving in communication over concatenating optimal encodings for the individual settings. This result also provides new non-linear coding schemes which improve upon the largest known gap between linear and non-linear Network Coding, thus improving the results of Dougherty, Freiling, and Zeger.
  The proofs use ideas related to Witsenhausen's rate, OR graph products, colorings of Cayley graphs and the chromatic numbers of Kneser graphs.
