Reading policies for joins: An asymptotic analysis

Suppose that $m_n$ observations are made from the distribution $\mathbf {R}$ and $n-m_n$ from the distribution $\mathbf {S}$. Associate with each pair, $x$ from $\mathbf {R}$ and $y$ from $\mathbf {S}$, a nonnegative score $φ(x,y)$. An optimal reading policy is one that yields a sequence $m_n$ that maximizes $\mathbb{E}(M(n))$, the expected sum of the $(n-m_n)m_n$ observed scores, uniformly in $n$. The alternating policy, which switches between the two sources, is the optimal nonadaptive policy. In contrast, the greedy policy, which chooses its source to maximize the expected gain on the next step, is shown to be the optimal policy. Asymptotics are provided for the case where the $\mathbf {R}$ and $\mathbf {S}$ distributions are discrete and $φ(x,y)=1 or 0$ according as $x=y$ or not (i.e., the observations match). Specifically, an invariance result is proved which guarantees that for a wide class of policies, including the alternating and the greedy, the variable M(n) obeys the same CLT and LIL. A more delicate analysis of the sequence $\mathbb{E}(M(n))$ and the sample paths of M(n), for both alternating and greedy, reveals the slender sense in which the latter policy is asymptotically superior to the former, as well as a sense of equivalence of the two and robustness of the former.
