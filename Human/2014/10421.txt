Impact of Received Signal on Self-interference Channel Estimation and Achievable Rates in In-band Full-duplex Transceivers

In this paper we analyze the effect of the calibration period, or lack of, on self-interference channel estimation in the digital domain of in-band full-duplex radio transceivers. In particular, we consider a scenario where the channel estimation must be performed without a separate calibration period, which means that the received signal of interest will act as an additional noise source from the estimation perspective. We will explicitly analyze its effect, and quantify the increase in the parameter estimation variance, or sample size, if similar accuracy for the channel estimate is to be achieved as with a separate calibration period. In addition, we will analyze how the calibration period, or its absence, affects the overall achievable rates. Full waveform simulations are then used to determine the validity of the obtained results, as well as provide numerical results regarding the achievable rates. It is shown that, even though a substantial increase in the parameter sample size is required if there is no calibration period, the achievable rates are still comparable for the two scenarios.
