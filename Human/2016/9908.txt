On the entropy of a noisy function

Let $0 < ε< 1/2$ be a noise parameter, and let $T_ε$ be the noise operator acting on functions on the boolean cube $\{0,1\}^n$. Let $f$ be a nonnegative function on $\{0,1\}^n$. We upper bound the entropy of $T_ε f$ by the average entropy of conditional expectations of $f$, given sets of roughly $(1-2ε)^2 \cdot n$ variables. In information-theoretic terms, we prove the following strengthening of "Mrs. Gerber's lemma": Let $X$ be a random binary vector of length $n$, and let $Z$ be a noise vector, corresponding to a binary symmetric channel with crossover probability $ε$. Then, setting $v = (1-2ε)^2 \cdot n$, we have (up to lower-order terms): $$ H\Big(X \oplus Z\Big) \ge n \cdot H\left(ε~+~ (1-2ε) \cdot H^{-1}\left(\frac{{\mathbb E}_{|B| = v} H\Big(\{X_i\}_{i\in B}\Big)}{v}\right)\right) $$ As an application, we show that for a boolean function $f$, which is close to a characteristic function $g$ of a subcube of dimension $n-1$, the entropy of $T_ε f$ is at most that of $T_ε g$. This, combined with a recent result of Ordentlich, Shayevitz, and Weinstein shows that the "Most informative boolean function" conjecture of Courtade and Kumar holds for high noise $ε\ge 1/2 - δ$, for some absolute constant $δ> 0$. Namely, if $X$ is uniformly distributed in $\{0,1\}^n$ and $Y$ is obtained by flipping each coordinate of $X$ independently with probability $ε$, then, provided $ε\ge 1/2 - δ$, for any boolean function $f$ holds $I\Big(f(X);Y\Big) \le 1 - H(ε)$.
