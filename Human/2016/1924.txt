Randomized approximate nearest neighbor search with limited adaptivity

We study the fundamental problem of approximate nearest neighbor search in $d$-dimensional Hamming space $\{0,1\}^d$. We study the complexity of the problem in the famous cell-probe model, a classic model for data structures. We consider algorithms in the cell-probe model with limited adaptivity, where the algorithm makes $k$ rounds of parallel accesses to the data structure for a given $k$. For any $k\ge 1$, we give a simple randomized algorithm solving the approximate nearest neighbor search using $k$ rounds of parallel memory accesses, with $O(k(\log d)^{1/k})$ accesses in total. We also give a more sophisticated randomized algorithm using $O(k+(\frac{1}{k}\log d)^{O(1/k)})$ memory accesses in $k$ rounds for large enough $k$. Both algorithms use data structures of size polynomial in $n$, the number of points in the database.
  For the lower bound, we prove an $Ω(\frac{1}{k}(\log d)^{1/k})$ lower bound for the total number of memory accesses required by any randomized algorithm solving the approximate nearest neighbor search within $k\le\frac{\log\log d}{2\log\log\log d}$ rounds of parallel memory accesses on any data structures of polynomial size. This lower bound shows that our first algorithm is asymptotically optimal for any constant round $k$. And our second algorithm approaches the asymptotically optimal tradeoff between rounds and memory accesses, in a sense that the lower bound of memory accesses for any $k_1$ rounds can be matched by the algorithm within $k_2=O(k_1)$ rounds. In the extreme, for some large enough $k=Θ\left(\frac{\log\log d}{\log\log\log d}\right)$, our second algorithm matches the $Θ\left(\frac{\log\log d}{\log\log\log d}\right)$ tight bound for fully adaptive algorithms for approximate nearest neighbor search due to Chakrabarti and Regev.
