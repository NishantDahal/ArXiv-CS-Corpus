Quickest Change Detection with Mismatched Post-Change Models

In this paper, we study the quickest change detection with mismatched post-change models. A change point is the time instant at which the distribution of a random process changes. The objective of quickest change detection is to minimize the detection delay of an unknown change point under certain performance constraints, such as average run length (ARL) to false alarm or probability of false alarm (PFA). Most existing change detection procedures assume perfect knowledge of the random process distributions before and after the change point. However, in many practical applications such as anomaly detection, the post-change distribution is often unknown and needs to be estimated with a limited number of samples. In this paper, we study the case that there is a mismatch between the true post-change distribution and the one used during detection. We analytically identify the impacts of mismatched post-change models on two classical detection procedures, the cumulative sum (CUSUM) procedure and the Shiryaev-Roberts (SR) procedure. The impacts of mismatched models are characterized in terms of various finite or asymptotic performance bounds on ARL, PFA, and average detection delay (ADD). It is shown that post-change model mismatch results in an increase in ADD, and the rate of performance degradation depends on the difference between two Kullback-Leibler (KL) divergences, one is between the priori- and post-change distributions, and the other one is between the true and mismatched post-change distributions.
