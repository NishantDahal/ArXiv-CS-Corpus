A General Theory of Information and Computation

This paper fills a gap in our understanding of the interaction between information and computation. It unifies other approaches to measuring information like Kolmogorov complexity and Shannon information. We define a theory about information flow in deterministic computing based on three fundamental observations: 1) Information is measured in logarithms, 2) All countable sets contain the same amount of information and 3) Deterministic computing does not create information. We analyze the flow of information through computational processes: exactly, for primitive recursive functions and elementary artithmetical operations and, under maximal entropy, for polynomial functions and diophantine equations. Thus we get, by the MRDP-theorem, a theory of flow of information for general computable functions. We prove some results like the Fueter-PÃ³lya conjecture and the existence of an information conserving enumeration of all finite sets of numbers. We also show that the information flow in more complex derivatives of the primitive recursive functions like addition and multiplication is not trivial: in particular associativity is not information efficient for addition. Using the Cantor pairing function we develop a universal measuring device for partitions of the set of finite sets of numbers. We show that these sets can be enumerated by a polynomial function when ordered by cardinality, but not when ordered by their sums.
