In the Search of Optimal Concurrency

Implementing a concurrent data structure typically begins with defining its sequential specification. However, when used \emph{as is}, a nontrivial sequential data structure, such as a linked list, a search tree, or a hash table, may expose incorrect behavior: lost updates, inconsistent responses, etc. To ensure correctness, portions of the sequential code operating on the shared data must be "protected" from data races using synchronization primitives and, thus, certain schedules of the steps of concurrent operations must be rejected. But can we ensure that we do not "overuse" synchronization, i.e., that we reject a concurrent schedule only if it violates correctness?
  In this paper, we treat this question formally by introducing the notion of a \emph{concurrency-optimal} implementation. A program's concurrency is defined here as its ability to accept concurrent schedules, i.e., interleavings of steps of its sequential implementation. An implementation is concurrency-optimal if it accepts all interleavings that do not violate the program's correctness. We explore the concurrency properties of \emph{search} data structures which can be represented in the form of directed acyclic graphs exporting insert, delete and search operations. We prove, for the first time, that \emph{pessimistic} e.g., based on conservative locking) and \emph{optimistic serializable} e.g., based on serializable transactional memory) implementations of search data-structures are incomparable in terms of concurrency. Specifically, there exist simple interleavings of sequential code that cannot be accepted by \emph{any} pessimistic (and \emph{resp.}, serializable optimistic) implementation, but accepted by a serializable optimistic one (and \emph{resp.}, pessimistic). Thus, neither of these two implementation classes is concurrency-optimal.
