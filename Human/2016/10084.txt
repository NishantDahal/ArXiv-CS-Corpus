A Unified Paradigm of Organized Complexity and Semantic Information Theory

One of the most fundamental problems in science is to define {\it quantitatively} the complexity of organized matters, i.e., {\it organized complexity}. Although many measures have been proposed toward this aim in previous decades, there is no agreed upon definition. This paper presents a new quantitative definition of organized complexity. This definition {\it simultaneously} captures the three major features of complexity: computational (similar to logical depth), descriptional (similar to the Kolmogorov complexity and effective complexity) and distributional (similar to statistical complexity). In addition, the proposed definition is computable and can measure both probabilistic and deterministic forms of objects in a unified manner. The proposed definition is based on circuits rather than Turing machines and $Îµ$-machines. We give several criteria required for organized complexity measures and show that the proposed definition satisfies all of them for the first time. We then apply this quantitative definition to formulate a {\it semantic information theory}. We present the first formal definition of a {\it semantic information amount}, which is the core concept of the semantic information theory, that is based only on concretely defined notions. Previous semantic information theories defined this amount under some a priori information which is not concretely specified. We then unveil several fundamental properties in the semantic information theory, e.g., a semantic source coding theorem, semantic channel coding theorem, and effectiveness coding theorem. Although the semantic information theory has a long history of research going back more than six decades, there has been no study on its relation to organized complexity. This paper offers the first unified paradigm of organized complexity and semantic information theory.
