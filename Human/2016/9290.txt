Cutsize Distributions of Balanced Hypergraph Bipartitions for Random Hypergraphs

In a previous work, we presented a parallel encoding algorithm for low-density parity-check (LDPC) codes by partitioning hypergraph representation for the LDPC codes. The aim of this research is to analyze the processing time of this encoding algorithm. This paper clarifies that the processing time of the encoding algorithm depends on the minimum cutsize of balanced hypergraph partitions. Moreover, this paper gives the typical minimum cutsize and cutsize distribution for balanced hypergraph bipartitions of random hypergraphs defined from a regular LDPC ensemble.
