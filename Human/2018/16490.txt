Reducing Deep Network Complexity with Fourier Transform Methods

We propose a novel way that uses shallow densely connected neuron network architectures to achieve superior performance to convolution based neural networks (CNNs) approaches with the added benefits of lower computation burden requiring dramatically less training examples to achieve high prediction accuracy ($>98\%$). The advantages of our proposed method is demonstrated in results on benchmark datasets which show significant performance gain over existing state-of-the-art results on MNIST, CIFAR-10 and CIFAR-100. By Fourier transforming the inputs, each point in the training sample then has a representational energy of all the weighted information from every other point. The consequence of using this input is a reduced complexity neuron network, reduced computation load and the lifting of the requirement for a large number of training examples to achieve high classification accuracy.
