Minimax Optimal Additive Functional Estimation with Discrete Distribution: Slow Divergence Speed Case

This paper addresses an estimation problem of an additive functional of $φ$, which is defined as $θ(P;φ)=\sum_{i=1}^kφ(p_i)$, given $n$ i.i.d. random samples drawn from a discrete distribution $P=(p_1,...,p_k)$ with alphabet size $k$. We have revealed in the previous paper that the minimax optimal rate of this problem is characterized by the divergence speed of the fourth derivative of $φ$ in a range of fast divergence speed. In this paper, we prove this fact for a more general range of the divergence speed. As a result, we show the minimax optimal rate of the additive functional estimation for each range of the parameter $α$ of the divergence speed. For $α\in (1,3/2)$, we show that the minimax rate is $\frac{1}{n}+\frac{k^2}{(n\ln n)^{2α}}$. Besides, we show that the minimax rate is $\frac{1}{n}$ for $α\in [3/2,2]$.
