Minimax Optimal Additive Functional Estimation with Discrete Distribution

This paper addresses a problem of estimating an additive functional given $n$ i.i.d. samples drawn from a discrete distribution $P=(p_1,...,p_k)$ with alphabet size $k$. The additive functional is defined as $θ(P;φ)=\sum_{i=1}^kφ(p_i)$ for a function $φ$, which covers the most of the entropy-like criteria. The minimax optimal risk of this problem has been already known for some specific $φ$, such as $φ(p)=p^α$ and $φ(p)=-p\ln p$. However, there is no generic methodology to derive the minimax optimal risk for the additive function estimation problem. In this paper, we reveal the property of $φ$ that characterizes the minimax optimal risk of the additive functional estimation problem; this analysis is applicable to general $φ$. More precisely, we reveal that the minimax optimal risk of this problem is characterized by the divergence speed of the function $φ$.
