Ethical Implications: The ACM/IEEE-CS Software Engineering Code applied to Tesla's "Autopilot" System

On October 14, 2015, Tesla Inc. an American electric car company, released the initial version of the Autopilot system. This system promised to provide semi-autonomous driving using the existing hardware already installed on Tesla vehicles. On March 23rd, 2018, a Tesla vehicle ran into a divider at highway speed, killing the driver. This occurred under the control of the Autopilot system with no driver intervention. Critics argue that though Tesla gives drivers warnings in its owner's manual, it is ultimately unethical to release a system that is marketed as an Autopilot yet still makes grave mistakes that any human driver would not make. Others defend Tesla by stating that their advisories are suitable and that drivers should ultimately be at fault for any mistakes of the Autopilot. This paper will scrutinize the ethical implications of Tesla's choice to develop, market, and ship a beta product that requires extensive testing. It will further analyze the implications of Tesla's aggressive advertisement of the product under the name Autopilot along with associated marketing materials. By applying the joint ACM/IEEE-CS Software Engineering Code of Ethics, this paper will show that Tesla's choices and actions during this event are inconsistent with the code and are unethical since they are responsible for adequately testing and honestly marketing their product.
