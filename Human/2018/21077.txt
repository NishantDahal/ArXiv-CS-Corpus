On the compression of messages in the multi-party setting

We consider the following communication task in the multi-party setting, which involves a joint random variable $XYZMN$ with the property that $M$ is independent of $YZN$ conditioned on $X$ and $N$ is independent of $XZM$ conditioned on $Y$. Three parties Alice, Bob and Charlie, respectively, observe samples $x,y$ and $z$ from $XYZ$. Alice and Bob communicate messages to Charlie with the goal that Charlie can output a sample from $MN$ having correct correlation with $XYZ$. This task reflects the simultaneous message passing model of communication complexity. Furthermore, it is a generalization of some well studied problems in information theory, such as distributed source coding, source coding with a helper and one sender and one receiver message compression. It is also closely related to the lossy distributed source coding task.
  Our main result is an achievable communication region for this task in the one-shot setting, through which we obtain a near optimal characterization using auxiliary random variables of bounded size. We employ our achievability result to provide a near-optimal one-shot communication region for the task of lossy distributed source coding, in terms of auxiliary random variables of bounded size. Finally, we show that interaction is necessary to achieve the optimal expected communication cost for our main task.
