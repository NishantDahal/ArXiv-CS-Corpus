Norm-Free Radon-Nikodym Approach to Machine Learning

For Machine Learning (ML) classification problem, where a vector of $\mathbf{x}$--observations (values of attributes) is mapped to a single $y$ value (class label), a generalized Radon--Nikodym type of solution is proposed. Quantum--mechanics --like probability states $ψ^2(\mathbf{x})$ are considered and "Cluster Centers", corresponding to the extremums of $<yψ^2(\mathbf{x})>/<ψ^2(\mathbf{x})>$, are found from generalized eigenvalues problem. The eigenvalues give possible $y^{[i]}$ outcomes and corresponding to them eigenvectors $ψ^{[i]}(\mathbf{x})$ define "Cluster Centers". The projection of a $ψ$ state, localized at given $\mathbf{x}$ to classify, on these eigenvectors define the probability of $y^{[i]}$ outcome, thus avoiding using a norm ($L^2$ or other types), required for "quality criteria" in a typical Machine Learning technique. A coverage of each `Cluster Center" is calculated, what potentially allows to separate system properties (described by $y^{[i]}$ outcomes) and system testing conditions (described by $C^{[i]}$ coverage). As an example of such application $y$ distribution estimator is proposed in a form of pairs $(y^{[i]},C^{[i]})$, that can be considered as Gauss quadratures generalization. This estimator allows to perform $y$ probability distribution estimation in a strongly non--Gaussian case.
