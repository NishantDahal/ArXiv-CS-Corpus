Exposing ambiguities in a relation-extraction gold standard with crowdsourcing

Semantic relation extraction is one of the frontiers of biomedical natural language processing research. Gold standards are key tools for advancing this research. It is challenging to generate these standards because of the high cost of expert time and the difficulty in establishing agreement between annotators. We implemented and evaluated a microtask crowdsourcing approach that can produce a gold standard for extracting drug-disease relations. The aggregated crowd judgment agreed with expert annotations from a pre-existing corpus on 43 of 60 sentences tested. The levels of crowd agreement varied in a similar manner to the levels of agreement among the original expert annotators. This work rein-forces the power of crowdsourcing in the process of assembling gold standards for relation extraction. Further, it high-lights the importance of exposing the levels of agreement between human annotators, expert or crowd, in gold standard corpora as these are reproducible signals indicating ambiguities in the data or in the annotation guidelines.
