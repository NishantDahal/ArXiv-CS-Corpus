Learning Representations from Deep Networks Using Mode Synthesizers

Deep learning Networks play a crucial role in the evolution of a vast number of current machine learning models for solving a variety of real world non-trivial tasks. Such networks use big data which is generally unlabeled unsupervised and multi-layered requiring no form of supervision for training and learning data and has been used to successfully build automatic supervisory neural networks. However the question still remains how well the learned data represents interestingness, and their effectiveness i.e. efficiency in deep learning models or applications. If the output of a network of deep learning models can be beamed unto a scene of observables, we could learn the variational frequencies of these stacked networks in a parallel and distributive way.This paper seeks to discover and represent interesting patterns in an efficient and less complex way by incorporating the concept of Mode synthesizers in the deep learning process models
