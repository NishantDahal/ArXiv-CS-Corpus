A memory versus compression ratio trade-off in PPM via compressed context modeling

Since its introduction prediction by partial matching (PPM) has always been a de facto gold standard in lossless text compression, where many variants improving the compression ratio and speed have been proposed. However, reducing the high space requirement of PPM schemes did not gain that much attention. This study focuses on reducing the memory consumption of PPM via the recently proposed compressed context modeling that uses the compressed representations of contexts in the statistical model. Differently from the classical context definition as the string of the preceding characters at a particular position, CCM considers context as the amount of preceding information that is actually the bit stream composed by compressing the previous symbols. We observe that by using the CCM, the data structures, particularly the context trees, can be implemented in smaller space, and present a trade-off between the compression ratio and the space requirement. The experiments conducted showed that this trade-off is especially beneficial in low orders with approximately 20 - 25 percent gain in memory by a sacrifice of up to nearly 7 percent loss in compression ratio.
