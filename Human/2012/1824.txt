A Convex Formulation for Learning Task Relationships in Multi-Task Learning

Multi-task learning is a learning paradigm which  seeks to improve the generalization performance  of a learning task with the help of some other related  tasks. In this paper, we propose a regularization  formulation for learning the relationships  between tasks in multi-task learning. This formulation  can be viewed as a novel generalization  of the regularization framework for single-task  learning. Besides modeling positive task correlation,  our method, called multi-task relationship  learning (MTRL), can also describe negative  task correlation and identify outlier tasks  based on the same underlying principle. Under  this regularization framework, the objective  function of MTRL is convex. For efficiency,  we use an alternating method to learn the optimal  model parameters for each task as well  as the relationships between tasks. We study  MTRL in the symmetric multi-task learning setting  and then generalize it to the asymmetric setting  as well. We also study the relationships between  MTRL and some existing multi-task learning  methods. Experiments conducted on a toy  problem as well as several benchmark data sets  demonstrate the effectiveness of MTRL.
