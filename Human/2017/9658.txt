Population protocols for leader election and exact majority with O(log^2 n) states and O(log^2 n) convergence time

We consider the model of population protocols, which can be viewed as a sequence of random pairwise interactions of $n$ agents (nodes). We show population protocols for two problems: the leader election and the exact majority voting. The leader election starts with all agents in the same initial state and the goal is to converge to the (global) state when exactly one agent is in a distinct state $L$. The exact majority voting starts with each agent in one of the two distinct states $A$ or $B$ and the goal is to make all nodes know which of these two states was the initial majority state, even if that majority was just by a single vote.
  Alistarh and Gelashvili [ICALP 2015] showed a leader-election protocol which converges in $O(\log^3 n)$ time w.h.p. and in expectation and needs $Θ(\log^3 n)$ states per agent. We present a protocol which elects the leader in $O(\log^2 n)$ time w.h.p. and in expectation and uses $Θ(\log^2 n)$ states per agent. For the exact majority voting, we show a population protocol with the same asymptotic performance: $O(\log^2 n)$ time and $Θ(\log^2 n)$ states per agent. The exact-majority protocol proposed by Alistarh et al. [PODC 2015] achieves expected $O(\log^2 n)$ time, but requires a relatively high initial imbalance between $A$'s and $B$'s or a large number of states per agent. More recently, Alistarh et al. [SODA 2017] showed $O(\log^2 n)$-state protocols for both problems, with the exact majority protocol converging in time $O(\log^3 n)$, and the leader election protocol converging in time $O(\log^{6.3} n)$ w.h.p. and $O(\log^{5.3} n)$ in expectation.
  Our leader election and exact majority protocols are based on the idea of agents counting their local interactions and rely on the probabilistic fact that the uniform random selection would limit the divergence of the individual counts.
