Differential Private Noise Adding Mechanism and Its Application on Consensus

Differential privacy is a formal mathematical {stand-ard} for quantifying the degree of that individual privacy in a statistical database is preserved. To guarantee differential privacy, a typical method is adding random noise to the original data for data release. In this paper, we investigate the conditions of differential privacy considering the general random noise adding mechanism, and then apply the obtained results for privacy analysis of the privacy-preserving consensus algorithm. Specifically, we obtain a necessary and sufficient condition of $ε$-differential privacy, and the sufficient conditions of $(ε, δ)$-differential privacy. We apply them to analyze various random noises. For the special cases with known results, our theory matches with the literature; for other cases that are unknown, our approach provides a simple and effective tool for differential privacy analysis. Applying the obtained theory, on privacy-preserving consensus algorithms, it is proved that the average consensus and $ε$-differential privacy cannot be guaranteed simultaneously by any privacy-preserving consensus algorithm.
