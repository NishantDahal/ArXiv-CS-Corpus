A Macroscopic Model for Differential Privacy in Dynamic Robotic Networks

The increasing availability of online and mobile information platforms is facilitating the development of peer-to-peer collaboration strategies in large-scale networks. These technologies are being leveraged by networked robotic systems to provide applications of automated transport, resource redistribution (collaborative consumption), and location services. Yet, external observations of the system dynamics may expose sensitive information about the participants that compose these networks (robots, resources, and humans). In particular, we are concerned with settings where an adversary gains access to a snapshot of the dynamic state of the system. We propose a method that quantifies how easy it is for the adversary to identify the specific type of any agent (which can be a robot, resource, or human) in the network, based on this observation. We draw from the theory of differential privacy to propose a closed-form expression for the leakage of the system when the snapshot is taken at steady-state, as well as a numerical approach to compute the leakage when the snapshot is taken at any given time. The novelty of our approach is that our privacy model builds on a macroscopic description of the system's state, which allows us to take account of protected entities (network participants) that are interdependent. Our results show how the leakage varies, as a function of the composition and dynamic behavior of the network; they also indicate design rules for increasing privacy levels.
