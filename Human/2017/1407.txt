Disambiguating the role of noise correlations when decoding neural populations together

One of the most controversial problems in neural decoding is quantifying the information loss caused by ignoring noise correlations during optimal brain computations. For more than a decade, the measure here called $ ΔI^{DL} $ has been believed exact. However, we have recently shown that it can exceed the information loss $ ΔI^{B} $ caused by optimal decoders constructed ignoring noise correlations. Unfortunately, the different information notions underlying $ ΔI^{DL} $ and $ ΔI^{B} $, and the putative rigorous information-theoretical derivation of $ ΔI^{DL} $, both render unclear whether those findings indicate either flaws in $ ΔI^{DL} $ or major departures from traditional relations between information and decoding. Here we resolve this paradox and prove that, under certain conditions, observing $ ΔI^{DL} {>}ΔI^{B} $ implies that $ ΔI^{DL} $ is flawed. Motivated by this analysis, we test both measures using neural populations that transmit independent information. Our results show that $ ΔI^{DL} $ may deem noise correlations more important when decoding the populations together than when decoding them in parallel, whereas the opposite may occur for $ ΔI^{B} $. We trace these phenomena back, for $ ΔI^{B} $, to the choice of tie-breaking rules, and for $ ΔI^{DL} $, to unforeseen limitations within its information-theoretical foundations. Our study contributes with better estimates that potentially improve theoretical and experimental inferences currently drawn from $ ΔI^{DL} $ without noticing that it may constitute an upper bound. On the practical side, our results promote the design of optimal decoding algorithms and neuroprosthetics without recording noise correlations, thereby saving experimental and computational resources.
