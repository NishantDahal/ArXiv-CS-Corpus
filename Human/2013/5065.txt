Determinism, Complexity, and Predictability in Computer Performance

Computers are deterministic dynamical systems (CHAOS 19:033124, 2009). Among other things, that implies that one should be able to use deterministic forecast rules to predict their behavior. That statement is sometimes-but not always-true. The memory and processor loads of some simple programs are easy to predict, for example, but those of more-complex programs like compilers are not. The goal of this paper is to determine why that is the case. We conjecture that, in practice, complexity can effectively overwhelm the predictive power of deterministic forecast models. To explore that, we build models of a number of performance traces from different programs running on different Intel-based computers. We then calculate the permutation entropy-a temporal entropy metric that uses ordinal analysis-of those traces and correlate those values against the prediction success
