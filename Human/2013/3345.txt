Minimal Assumption Distribution Propagation in Belief Networks

As belief networks are used to model increasingly complex situations, the need to automatically construct them from large databases will become paramount.  This paper concentrates on solving a part of the belief network induction problem: that of learning the quantitative structure (the conditional probabilities), given the qualitative structure.  In particular, a theory is presented that shows how to propagate inference distributions in a belief network, with the only assumption being that the given qualitative structure is correct.  Most inference algorithms must make at least this assumption.  The theory is based on four network transformations that are sufficient for any inference in a belief network.  Furthermore, the claim is made that contrary to popular belief, error will not necessarily grow as the inference chain grows.  Instead, for QBN belief nets induced from large enough samples, the error is more likely to decrease as the size of the inference chain increases.
